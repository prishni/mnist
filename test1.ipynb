{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import train\n",
    "def load_mnist():\n",
    "    data_dir = '../data'\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000)).astype(np.int)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000)).astype(np.int)\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    perm = np.random.permutation(trY.shape[0])\n",
    "    trX = trX[perm]\n",
    "    trY = trY[perm]\n",
    "\n",
    "    perm = np.random.permutation(teY.shape[0])\n",
    "    teX = teX[perm]\n",
    "    teY = teY[perm]\n",
    "\n",
    "    return trX, trY, teX, teY\n",
    "\n",
    "\n",
    "def print_digit(digit_pixels, label='?'):\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if digit_pixels[i, j] > 128:\n",
    "                print('#')\n",
    "            else:\n",
    "                print ('.')\n",
    "        print ('')\n",
    "\n",
    "    print ('Label: ', label)\n",
    "\n",
    "print(\"executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "biases =[]\n",
    "weights=[]\n",
    "def cost_derivative(output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "class neural_net(object):\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        global biases\n",
    "        global weights\n",
    "        self.size = sizes\n",
    "        for i in sizes[1:]:\n",
    "            bias = np.random.randn(i,1)\n",
    "            biases.append(bias)\n",
    "        for i,j in zip(sizes[1:3],sizes[:2]):\n",
    "            weight = np.random.randn(i,j)\n",
    "            weights.append(weight)\n",
    "        #print(weights)\n",
    "    def feedforward(self,activation):\n",
    "        global biases\n",
    "        global weights\n",
    "        zs=[]\n",
    "        As=[]\n",
    "        activation = activation.reshape(28,28)\n",
    "        activation  = activation.reshape(784,1)\n",
    "        for i in range(0,2):\n",
    "            w=weights[i]\n",
    "            b =  biases[i]\n",
    "            z=np.dot(w,activation)+b\n",
    "            #bigfloat.exp(5000,bigfloat.precision(100))\n",
    "            activation = 1/(1+np.exp(-1*z))\n",
    "            As.append(activation)\n",
    "            zs.append(z)\n",
    "        #print(np.argmax(As[-1]))\n",
    "        return zs,As\n",
    "        #print(As[-1],max(As[-1]),np.argmax(As[-1])) \n",
    "    def feedforwardtest(self,activation):\n",
    "        global biases\n",
    "        global weights\n",
    "        zs=[]\n",
    "        As=[]\n",
    "        activation = activation.reshape(28,28)\n",
    "        activation  = activation.reshape(784,1)\n",
    "        for i in range(0,2):\n",
    "            w=weights[i]\n",
    "            b =  biases[i]\n",
    "            z=np.dot(w,activation)+b\n",
    "            #bigfloat.exp(5000,bigfloat.precision(100))\n",
    "            activation = 1/(1+np.exp(-1*z))\n",
    "            As.append(activation)\n",
    "            zs.append(z)\n",
    "        #print(np.argmax(As[-1]))\n",
    "        return As[-1]\n",
    "    def backprop(self,x,y):\n",
    "        global biases\n",
    "        global weights\n",
    "        zs,As = self.feedforward(x)\n",
    "        delta = cost_derivative(As[-1],y) * sigmoid_prime(zs[-1])\n",
    "        update_bias = [np.zeros(b.shape) for b in  biases]\n",
    "        update_weight = [np.zeros(w.shape) for w in weights]\n",
    "        update_bias[-1] =delta\n",
    "        update_weight[-1]  = np.dot(delta, As[-2].transpose())\n",
    "        for layer in range(2,2):\n",
    "            z = zs[-layer]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(weights[-layer+1].transpose(), delta) * sp\n",
    "            update_bias[-layer] = delta\n",
    "            update_weight[-layer] = np.dot(delta, As[-layer-1].transpose())\n",
    "            #print(\"**********************\")\n",
    "            #print(update_bias)\n",
    "        return (update_bias, update_weight)\n",
    "    \n",
    "    def create_mini_batches(self,training_data_x,training_data_y):\n",
    "        eta = 3.0\n",
    "        mini_batch_size = 10\n",
    "        epochs = 30\n",
    "        mini_batches_x =[]\n",
    "        mini_batches_y =[]\n",
    "        n = len(training_data_x)\n",
    "        for j in range(epochs):\n",
    "            for k in range(0,n,mini_batch_size):\n",
    "                mini_batch_x = training_data_x[k:k+mini_batch_size]\n",
    "                mini_batch_y = training_data_y[k:k+mini_batch_size]\n",
    "                mini_batches_x.append(mini_batch_x)\n",
    "                mini_batches_y.append(mini_batch_y)\n",
    "            \n",
    "            for mini_batch_x,mini_batch_y in zip(mini_batches_x,mini_batches_y):\n",
    "                #print(\"next mini batch\")\n",
    "                self.update_mini_batch(mini_batch_x,mini_batch_y, eta)\n",
    "   \n",
    "    def update_mini_batch(self,mini_batch_x,mini_batch_y,eta):\n",
    "        global biases\n",
    "        global weights\n",
    "        #newbias =[]\n",
    "        #newweight=[]\n",
    "        #print(biases[0].shape)\n",
    "        #print(biases[1].shape)\n",
    "        newbias = [np.zeros(b.shape) for b in  biases]\n",
    "        newweight = [np.zeros(w.shape) for w in weights]\n",
    "        '''for b,w in ( biases,weights):\n",
    "            tempb = np.zeros(b.shape)\n",
    "            tempw = np.zeros(w.shape)\n",
    "            print(\"###\")\n",
    "            print(tempb.shape)\n",
    "            newbias.append(tempb)\n",
    "            newweight.append(tempw)'''\n",
    "\n",
    "        for x,y in zip(mini_batch_x,mini_batch_y):\n",
    "            delta_b ,delta_w = self.backprop(x,y)\n",
    "            #print(\"#####\")\n",
    "            #print(delta_w[0].shape,delta_w[1].shape)\n",
    "            #print(newweight[0].shape,newweight[1].shape)\n",
    "            #print(delta_b[0].shape,delta_b[1].shape)\n",
    "            i=0\n",
    "            for del_b,b in zip(newbias,delta_b):\n",
    "                newbias[i] = del_b + b\n",
    "                i = i+1\n",
    "            i=0\n",
    "            for del_w,w in zip(newweight,delta_w):\n",
    "                newweight[i] = del_w + w\n",
    "                i=i+1\n",
    "        new_final_weights=[]\n",
    "        for each_w, del_w in zip(weights,newweight):\n",
    "            weight1 = each_w - (eta/ len(mini_batch_x))*del_w\n",
    "            #print(weight1.shape)\n",
    "            #weight1[weight1>2.7] =2.7\n",
    "            #weight1[weight1<(-2.7)] =-2.7\n",
    "            new_final_weights.append(weight1)\n",
    "        print(new_final_weights)\n",
    "        weights = new_final_weights\n",
    "\n",
    "        new_final_biases=[]\n",
    "        for each_b, del_b in  zip(biases,newbias):\n",
    "            bias1 = each_b - (eta/ len(mini_batch_x))*del_b\n",
    "            new_final_biases.append(bias1)\n",
    "        biases = new_final_biases\n",
    "\n",
    "    def testFunction(self,testX,testY):\n",
    "        test_results = [(np.argmax(self.feedforwardtest(x)), y)\n",
    "                        for (x, y) in zip(testX,testY)]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "print(\"executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:39: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:57: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = load_mnist()\n",
    "#print \"Shapes: \", trainX.shape, trainY.shape, testX.shape, testY.shape\n",
    "#print \"\\nDigit sample\"\n",
    "#print_digit(trainX[1], trainY[1])\n",
    "net = neural_net([784, 10, 10])\n",
    "'''for x,y in zip(trainX[1;200],trainY[1:200]):\n",
    "    #print(x,y)\n",
    "    x = x.reshape(28,28)\n",
    "    x  = x.reshape(784,1)\n",
    "    print(y)'''\n",
    "    #print weights[2]\n",
    "#print ( biases[1])\n",
    "net.create_mini_batches(trainX[1:100],trainY[1:100])\n",
    "#print weights[2]\n",
    "#fp = open(\"biases.txt\",'w')\n",
    "#fp.write(biases)\n",
    "#f = open(\"weights.txt\",'w')\n",
    "#f.write(weights)\n",
    "\n",
    "print(net.testFunction(testX[1:100],testY[1:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:56: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854\n"
     ]
    }
   ],
   "source": [
    "print(net.testFunction(testX[1:5000],testY[1:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = neural_net([784, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.0142418 , -2.69242686,  0.05610535, ...,  0.65877382,\n",
      "         0.87077277,  0.49158738],\n",
      "       [-0.59343245,  0.57939284, -0.28119945, ...,  0.34303576,\n",
      "        -0.77009764,  0.73797119],\n",
      "       [-0.3900438 , -0.288673  , -1.33619303, ...,  0.77852933,\n",
      "         0.12880325, -2.05081199],\n",
      "       ..., \n",
      "       [ 0.27931462,  1.05078512,  1.79751655, ...,  1.28652074,\n",
      "        -0.69917399,  0.38174565],\n",
      "       [ 0.48411152,  1.69152611,  0.89745016, ...,  0.75379368,\n",
      "        -0.46006362, -0.05631043],\n",
      "       [ 1.00201866,  0.49405585,  0.06367002, ...,  0.73525837,\n",
      "        -0.64207392,  0.2605051 ]]), array([[-0.59735705,  1.38928772,  0.04623828,  0.14039754, -0.54290045,\n",
      "         1.80197633, -1.8507679 ,  0.26169715, -2.32391436, -0.13482018],\n",
      "       [ 0.73122462, -1.44145217, -0.8504607 ,  0.1248716 , -1.2993191 ,\n",
      "        -0.61585451, -0.95044165,  0.96509276, -0.80455871,  0.3970687 ],\n",
      "       [-0.04935669,  0.71214397, -0.90715145,  1.58306359, -0.05812726,\n",
      "        -0.08079579, -0.79447406,  0.35618493, -0.62677288,  1.63493295],\n",
      "       [ 1.98883281, -0.46266798, -0.07463115, -1.35167621, -1.23973652,\n",
      "        -0.97016346, -0.82239993,  1.17865511,  0.38635558,  1.68249621],\n",
      "       [ 0.03936437, -0.20891281,  0.01078604, -0.91741251,  1.7166961 ,\n",
      "         0.76857184,  0.51381311,  1.23029976, -1.38990075,  0.03716228],\n",
      "       [ 1.26527959, -0.8116287 , -0.57318479, -0.21714528,  0.42950422,\n",
      "         0.33035753,  1.29920108,  0.41893469,  0.59674281,  0.36887128],\n",
      "       [-1.57613095,  2.90121297,  1.73650821,  0.50041011,  0.34314652,\n",
      "        -0.85402095,  0.0471088 , -0.21620888,  2.03111734, -1.33676938],\n",
      "       [ 0.30809942,  0.09030703,  0.16847792, -0.10134546,  0.8749861 ,\n",
      "         0.73720697,  0.12724585,  0.0181534 , -1.78024661, -0.1416097 ],\n",
      "       [ 0.50853069, -0.02255785, -0.08178886,  0.52233183, -0.05068513,\n",
      "        -0.97839101,  0.88338094, -0.57579095,  0.84288536, -1.46839519],\n",
      "       [ 0.28678998, -1.0326796 ,  0.25580886, -0.1335322 ,  0.40383272,\n",
      "         0.77375761,  0.02010944, -0.08006875, -0.01034287,  0.08618111]]), array([[ 0.82895618,  0.19022195,  1.37811714, ..., -1.63747421,\n",
      "        -0.19918524,  2.23378085],\n",
      "       [ 0.14724403, -1.0922673 , -0.86181587, ..., -0.25277577,\n",
      "         1.55278862, -0.81380829],\n",
      "       [-0.29123837, -0.60280436,  0.84754788, ...,  0.26910071,\n",
      "         0.86669793,  1.00803013],\n",
      "       ..., \n",
      "       [ 0.10598955,  1.97236772, -0.26436227, ..., -1.83686649,\n",
      "        -1.01158154, -0.61698047],\n",
      "       [-1.30904234, -0.03035725,  0.44526426, ..., -0.28331803,\n",
      "         0.07146375,  0.7524421 ],\n",
      "       [ 0.44743069,  0.72561944,  1.7616498 , ..., -1.47690238,\n",
      "        -0.55002233,  0.42850674]]), array([[-0.13584286, -0.79547632, -0.77473306,  1.9025333 , -0.18947509,\n",
      "         0.8626219 ,  0.85509752,  0.78448448,  0.26860168,  0.57422148],\n",
      "       [ 0.211546  ,  0.89823182,  1.21988486, -0.20653923,  0.65929581,\n",
      "        -0.51121388,  0.35344256, -0.11506916,  0.94438161,  1.53408986],\n",
      "       [ 0.8162902 , -1.70743985, -0.2762249 , -0.6861831 , -1.84170968,\n",
      "         0.57803742, -2.40890635, -1.34877032, -2.33157413, -0.96081207],\n",
      "       [-0.09418335, -0.1257191 , -0.04396209, -0.9318326 ,  1.10641995,\n",
      "        -1.34019781, -0.49627252, -0.1947615 , -0.34517774, -1.02702886],\n",
      "       [ 0.10802665,  0.74988552,  1.54658562, -1.48605659,  0.2785902 ,\n",
      "        -0.29658604, -1.85950413, -0.09916195, -0.36343579,  0.18604459],\n",
      "       [-1.8664285 , -0.25221662,  1.66440857, -0.10959174,  0.87568894,\n",
      "        -0.49488705, -2.00147147,  1.33147169, -0.40450998, -1.34701772],\n",
      "       [ 0.69634024, -0.626705  , -1.76903154, -0.01653139, -0.7788178 ,\n",
      "         0.95454088, -1.52486772,  0.20882096, -1.14984929, -1.94346124],\n",
      "       [ 0.33279576, -1.43752251,  1.75580828, -0.05942956,  0.26552414,\n",
      "        -0.09791217, -0.96589806,  0.58981146, -0.67872343, -2.03536784],\n",
      "       [ 0.95693318, -1.53172678,  0.71861573, -0.74825901,  1.23587809,\n",
      "         0.22477225, -0.04752422,  0.95365225, -0.41750233,  2.36747602],\n",
      "       [ 0.1933016 ,  1.18467461, -1.18325812,  0.59992007,  0.02762915,\n",
      "         0.13360942,  0.30560977, -1.53451185, -0.09622305,  0.73698338]]), array([[ 0.48338532,  0.1916466 , -2.35386544, ..., -0.39946345,\n",
      "         1.2750017 , -0.96674052],\n",
      "       [ 0.86980738, -1.33535375, -0.02124315, ..., -0.48843815,\n",
      "        -0.29762195, -0.58980844],\n",
      "       [-0.15568352,  1.0731554 , -0.47914825, ...,  0.28988379,\n",
      "         0.77646173,  0.69366818],\n",
      "       ..., \n",
      "       [ 2.4058032 , -0.94466578, -0.68312599, ...,  1.26815208,\n",
      "         0.95615214, -1.81238598],\n",
      "       [ 0.45383671, -0.34490828, -1.14213442, ...,  1.22072116,\n",
      "         1.01400031,  1.36948938],\n",
      "       [ 0.20265961,  0.17333578, -0.5559191 , ..., -0.81000921,\n",
      "         0.30653191,  0.77648976]]), array([[-2.03698561,  0.16533205, -0.23551799,  1.57041634, -0.07107872,\n",
      "         1.65898597,  0.07256892,  0.73757337, -0.06482491,  1.46277589],\n",
      "       [-0.73260803, -0.66794751, -0.31719055,  0.24234471,  0.51847248,\n",
      "        -0.08560999,  0.24917876, -1.27401246, -1.36020471,  0.36112453],\n",
      "       [ 2.32090432, -0.33289857, -0.9347567 ,  1.98994645,  0.00364279,\n",
      "        -0.44246731,  0.50399545, -0.44305526, -1.26644959, -0.88417135],\n",
      "       [-0.0916617 ,  1.16548857, -1.42735483,  0.17156167,  0.16657242,\n",
      "         1.60734194, -0.76584255, -0.89093924, -0.00771262, -1.70046   ],\n",
      "       [ 0.82239487,  0.34498147,  1.0861066 ,  1.01322632,  1.57597577,\n",
      "         0.2726989 , -1.18690069,  0.30038345, -0.36911683, -0.21888242],\n",
      "       [ 1.05618862,  0.36863162,  1.12563561,  1.1255492 , -2.75675352,\n",
      "         2.25289768,  0.37740858,  0.32966911, -0.316155  , -0.25977263],\n",
      "       [ 0.09689358,  2.05826451, -0.5227747 , -0.6695753 , -0.08958004,\n",
      "        -0.68416924,  0.32828479,  1.85388492,  1.39682999,  0.18966967],\n",
      "       [-0.89060609,  0.38753116,  0.70969662,  0.58504994,  0.35970502,\n",
      "         0.46671753,  0.69751637,  0.80430161,  0.29274946, -1.62165321],\n",
      "       [-0.12297449, -1.90987821,  0.00995914,  0.95812067,  0.26305487,\n",
      "         0.21295835,  1.26168767,  1.03102322, -1.35842682,  0.70457134],\n",
      "       [ 0.19218312,  0.1301233 ,  0.90470251,  1.0138326 , -0.52030423,\n",
      "         0.8336553 , -0.37965633, -0.23504351, -0.0971909 ,  0.08983079]]), array([[ 0.37481043, -0.23296157,  0.29666847, ...,  1.01546612,\n",
      "        -0.86881645,  0.04992754],\n",
      "       [ 0.7239497 , -0.25077868, -0.01409377, ..., -2.38180052,\n",
      "        -0.77305653,  1.17084476],\n",
      "       [ 0.49867185, -1.1708559 ,  0.1207366 , ...,  0.49804398,\n",
      "         0.96885936,  0.35074525],\n",
      "       ..., \n",
      "       [-0.2926582 , -0.36733609,  0.68748268, ..., -1.56124624,\n",
      "        -0.28900797, -0.59983801],\n",
      "       [ 0.27823253, -1.15454565, -0.68487367, ...,  0.14394554,\n",
      "         0.81415343,  0.53832773],\n",
      "       [-0.28968416, -0.97104142,  0.39783936, ...,  0.07193539,\n",
      "        -0.17628345,  1.46604017]]), array([[ 1.45040869,  0.09591318, -1.93365359,  0.5313234 ,  0.4404244 ,\n",
      "        -0.11196578,  1.41917727,  0.69446695, -0.91971044,  1.2997288 ],\n",
      "       [ 1.15531378,  1.17797542, -1.08788967,  0.27174557,  0.78297663,\n",
      "         1.02271386, -0.35643216,  0.24708082, -0.29297715,  0.29492389],\n",
      "       [-0.03009811,  0.82711353, -2.25704913, -1.19711121,  0.51187515,\n",
      "        -0.0355607 , -0.97802476, -0.48881969, -0.62014617, -0.20992308],\n",
      "       [ 0.73007445, -0.20234208,  0.12015477, -0.31116917, -0.96508536,\n",
      "         1.53751955, -0.96387259,  1.39669293,  0.80307977,  0.22895536],\n",
      "       [-0.77873679, -0.83965501,  0.22512404, -0.90942212, -1.67848619,\n",
      "        -0.0987248 ,  0.52137618, -0.08158008, -1.08184331,  0.22482635],\n",
      "       [ 0.25073183,  0.20887541,  0.59432707,  0.01172305, -0.04407442,\n",
      "        -1.83121755, -0.06824976,  2.47558153,  1.98303693, -1.92058133],\n",
      "       [ 1.5808123 , -1.2886441 , -1.57417501, -0.48480285,  1.75164439,\n",
      "         1.10673424,  0.30517612, -0.13927176, -0.73056213, -1.50267566],\n",
      "       [ 1.31638461, -0.16098022,  0.26574519,  0.39864908,  1.24748718,\n",
      "        -0.76673692, -0.34679579, -0.28803425,  0.524777  ,  0.0820153 ],\n",
      "       [ 0.97569409,  0.00980305, -0.54781124, -0.10621143,  0.66557091,\n",
      "         0.32467715, -0.35041785,  0.0563057 , -1.47044521, -0.05630581],\n",
      "       [-1.01672707,  0.72641664,  0.6045155 ,  0.38458202,  1.07251938,\n",
      "        -1.52555301, -0.6458837 ,  0.30599248, -0.98262482,  0.39801088]])]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(weights[0][12:16,12:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(weights[0][12:16,12:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
